{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 - Latent space interpolation with MusicVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to sample, interpolate and humanize a drums sequence\n",
    "using MusicVAE and various configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. First lets define a method to download a MusicVAE checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "\n",
    "def download_checkpoint(model_name: str,\n",
    "                        checkpoint_name: str,\n",
    "                        target_dir: str):\n",
    "    \"\"\"\n",
    "    Downloads a Magenta checkpoint to target directory.\n",
    "\n",
    "    Target directory target_dir will be created if it does not already exist.\n",
    "\n",
    "        :param model_name: magenta model name to download\n",
    "        :param checkpoint_name: magenta checkpoint name to download\n",
    "        :param target_dir: local directory in which to write the checkpoint\n",
    "    \"\"\"\n",
    "    tf.gfile.MakeDirs(target_dir)\n",
    "    checkpoint_target = os.path.join(target_dir, checkpoint_name)\n",
    "    if not os.path.exists(checkpoint_target):\n",
    "        response = urllib.request.urlopen(\n",
    "            f\"https://storage.googleapis.com/magentadata/models/\"\n",
    "            f\"{model_name}/checkpoints/{checkpoint_name}\")\n",
    "        data = response.read()\n",
    "        local_file = open(checkpoint_target, 'wb')\n",
    "        local_file.write(data)\n",
    "        local_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Lets now define a method to instantiate the model from a model's name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from magenta.models.music_vae import TrainedModel, configs\n",
    "\n",
    "def get_model(name: str):\n",
    "    \"\"\"\n",
    "    Returns the model instance from its name.\n",
    "\n",
    "        :param name: the model name\n",
    "    \"\"\"\n",
    "    checkpoint = name + \".tar\"\n",
    "    download_checkpoint(\"music_vae\", checkpoint, \"checkpoints\")\n",
    "    return TrainedModel(\n",
    "        # Removes the .lohl in some training checkpoint which shares the same config\n",
    "        configs.CONFIG_MAP[name.split(\".\")[0] if \".\" in name else name],\n",
    "        # The batch size changes the number of sequences to be processed together,\n",
    "        # we'll be working with maximum 6 sequences (during groove)\n",
    "        batch_size=8,\n",
    "        checkpoint_dir_or_path=os.path.join(\"checkpoints\", checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can now write our sample method, which will get the model, call the sample method on it, and then save the result in the output folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from magenta.protobuf.music_pb2 import NoteSequence\n",
    "\n",
    "from note_sequence_utils import save_midi, save_plot\n",
    "\n",
    "def sample(model_name: str,\n",
    "           num_steps_per_sample: int) -> List[NoteSequence]:\n",
    "    \"\"\"\n",
    "    Samples 2 sequences using the given model.\n",
    "    \"\"\"\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # Uses the model to sample 2 sequences,\n",
    "    # with the number of steps and default temperature\n",
    "    sample_sequences = model.sample(n=2, length=num_steps_per_sample, \n",
    "                                    temperature=1.1)\n",
    "\n",
    "    # Saves the midi and the plot in the sample folder\n",
    "    save_midi(sample_sequences, \"sample\", model_name)\n",
    "    save_plot(sample_sequences, \"sample\", model_name)\n",
    "\n",
    "    return sample_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now to the interpolate method, which takes 2 sequences en interpolates between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import magenta.music as mm\n",
    "\n",
    "def interpolate(model_name: str,\n",
    "                sample_sequences: List[NoteSequence],\n",
    "                num_steps_per_sample: int,\n",
    "                num_output: int,\n",
    "                total_bars: int) -> NoteSequence:\n",
    "    \"\"\"\n",
    "    Interpolates between 2 sequences using the given model.\n",
    "    \"\"\"\n",
    "    if len(sample_sequences) != 2:\n",
    "        raise Exception(f\"Wrong number of sequences, \"\n",
    "                        f\"expected: 2, actual: {len(sample_sequences)}\")\n",
    "    if not sample_sequences[0].notes or not sample_sequences[1].notes:\n",
    "        raise Exception(f\"Empty note sequences, \"\n",
    "                        f\"sequence 1 length: {len(sample_sequences[0].notes)}, \"\n",
    "                        f\"sequence 2 length: {len(sample_sequences[1].notes)}\")\n",
    "\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # Use the model to interpolate between the 2 input sequences,\n",
    "    # with the number of output (counting the start and end sequence),\n",
    "    # number of steps per sample and default temperature\n",
    "    #\n",
    "    # This might throw a NoExtractedExamplesError exception if the\n",
    "    # sequences are not properly formed (for example if the sequences\n",
    "    # are not quantized, a sequence is empty or not of the proper length).\n",
    "    interpolate_sequences = model.interpolate(\n",
    "        start_sequence=sample_sequences[0],\n",
    "        end_sequence=sample_sequences[1],\n",
    "        num_steps=num_output,\n",
    "        length=num_steps_per_sample)\n",
    "\n",
    "    # Saves the midi and the plot in the interpolate folder\n",
    "    save_midi(interpolate_sequences, \"interpolate\", model_name)\n",
    "    save_plot(interpolate_sequences, \"interpolate\", model_name)\n",
    "\n",
    "    # Concatenates the resulting sequences (of length num_output) into one\n",
    "    # single sequence.\n",
    "    # The second parameter is a list containing the number of seconds\n",
    "    # for each input sequence. This is useful if some of the input\n",
    "    # sequences do not have notes at the end (for example the last\n",
    "    # note ends at 3.5 seconds instead of 4)\n",
    "    interpolate_sequence = mm.sequences_lib.concatenate_sequences(\n",
    "        interpolate_sequences, [4] * num_output)\n",
    "\n",
    "    # Saves the midi and the plot in the merge folder,\n",
    "    # with the plot having total_bars size\n",
    "    save_midi(interpolate_sequence, \"merge\", model_name)\n",
    "    save_plot(interpolate_sequence, \"merge\", model_name,\n",
    "              plot_max_length_bar=total_bars,\n",
    "              bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])\n",
    "\n",
    "    return interpolate_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The final method to define is the groove method, which takes the interpolated sequence and adds groove to it, by splitting it in smaller chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groove(model_name: str,\n",
    "           interpolate_sequence: NoteSequence,\n",
    "           num_steps_per_sample: int,\n",
    "           num_output: int,\n",
    "           total_bars: int) -> NoteSequence:\n",
    "    \"\"\"\n",
    "    Adds groove to the given sequence by splitting it in manageable sequences\n",
    "    and using the given model to humanize it.\n",
    "    \"\"\"\n",
    "    model = get_model(model_name)\n",
    "\n",
    "    # Split the sequences in chunks of 4 seconds (which is 2 bars at 120 qpm),\n",
    "    # which is necessary since the model is trained for 2 bars\n",
    "    split_interpolate_sequences = mm.sequences_lib.split_note_sequence(\n",
    "        interpolate_sequence, 4)\n",
    "\n",
    "    if len(split_interpolate_sequences) != num_output:\n",
    "        raise Exception(f\"Wrong number of interpolate size, \"\n",
    "                        f\"expected: 10, actual: {split_interpolate_sequences}\")\n",
    "\n",
    "    # Uses the model to encode the list of sequences, returning the encoding\n",
    "    # (also called z or latent vector) which will the used in the decoding,\n",
    "    # The other values mu and sigma are not used, but kept in the code for\n",
    "    # clarity.\n",
    "    #\n",
    "    # The resulting array shape is (a, b), where a is the number of\n",
    "    # split sequences (should correspond to num_output), and b is the encoding\n",
    "    # size.\n",
    "    #\n",
    "    # This might throw a NoExtractedExamplesError exception if the\n",
    "    # sequences are not properly formed (for example if the sequences\n",
    "    # are not quantized, a sequence is empty or not of the proper length).\n",
    "    encoding, mu, sigma = model.encode(\n",
    "        note_sequences=split_interpolate_sequences)\n",
    "\n",
    "    # Uses the model to decode the encoding (also called z or latent vector),\n",
    "    # returning a list of humanized sequence with one element per encoded\n",
    "    # sequences (each of length num_steps_per_sample).\n",
    "    groove_sequences = model.decode(\n",
    "        z=encoding, length=num_steps_per_sample)\n",
    "\n",
    "    # Concatenates the resulting sequences (of length num_output) into one\n",
    "    # single sequence.\n",
    "    groove_sequence = mm.sequences_lib.concatenate_sequences(\n",
    "        groove_sequences, [4] * num_output)\n",
    "\n",
    "    # Saves the midi and the plot in the groove folder,\n",
    "    # with the plot having total_bars size\n",
    "    save_midi(groove_sequence, \"groove\", model_name)\n",
    "    save_plot(groove_sequence, \"groove\", model_name,\n",
    "              plot_max_length_bar=total_bars, show_velocity=True,\n",
    "              bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])\n",
    "\n",
    "    return groove_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We can now call our methods, but first let's define some variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magenta.music import DEFAULT_STEPS_PER_BAR\n",
    "\n",
    "# Number of interpolated sequences (counting the start and end sequences)\n",
    "num_output = 6\n",
    "\n",
    "# Number of bar per sample, also giving the size of the interpolation splits\n",
    "num_bar_per_sample = 2\n",
    "\n",
    "# Number of steps per sample and interpolation splits\n",
    "num_steps_per_sample = num_bar_per_sample * DEFAULT_STEPS_PER_BAR\n",
    "\n",
    "# The total number of bars\n",
    "total_bars = num_output * num_bar_per_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Lets call the sample method using the \"cat-drums_2bar_small.lokl\" configuration, which will generate 2 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 8, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/sample/cat-drums_2bar_small.lokl_00_2019-10-11_201810.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/sample/cat-drums_2bar_small.lokl_01_2019-10-11_201810.mid\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/sample/cat-drums_2bar_small.lokl_00_2019-10-11_201810.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/sample/cat-drums_2bar_small.lokl_01_2019-10-11_201810.html\n"
     ]
    }
   ],
   "source": [
    "# Samples 2 new sequences with \"lokl\" model which is optimized for sampling\n",
    "generated_sample_sequences = sample(\"cat-drums_2bar_small.lokl\",\n",
    "                                    num_steps_per_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of two samples: \n",
    "![MusicVAE sample 01](docs/img/music_vae_sample_01.png)\n",
    "![MusicVAE sample 02](docs/img/music_vae_sample_02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Lets now call the interpolate method using the \"cat-drums_2bar_small.hikl\" configuration, which will take the 2 generated samples and interpolate between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 8, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 1.0, 'sampling_schedule': 'inverse_sigmoid', 'sampling_rate': 1000, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "INFO:tensorflow:Unbundling checkpoint.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp_jx517s_/cat-drums_2bar_small.hikl.ckpt\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_00_2019-10-11_201811.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_01_2019-10-11_201811.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_02_2019-10-11_201811.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_03_2019-10-11_201811.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_04_2019-10-11_201811.mid\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_05_2019-10-11_201811.mid\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_00_2019-10-11_201812.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_01_2019-10-11_201812.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_02_2019-10-11_201812.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_03_2019-10-11_201812.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_04_2019-10-11_201812.html\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/interpolate/cat-drums_2bar_small.hikl_05_2019-10-11_201812.html\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/merge/cat-drums_2bar_small.hikl_00_2019-10-11_201812.mid\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/merge/cat-drums_2bar_small.hikl_00_2019-10-11_201812.html\n"
     ]
    }
   ],
   "source": [
    "# Interpolates between the 2 sequences, returns 1 sequence\n",
    "# with \"hikl\" which is optimized for sampling\n",
    "generated_interpolate_sequence = interpolate(\"cat-drums_2bar_small.hikl\",\n",
    "                                             generated_sample_sequences,\n",
    "                                             num_steps_per_sample,\n",
    "                                             num_output,\n",
    "                                             total_bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of an interpolation: \n",
    "![MusicVAE interpolate 01](docs/img/music_vae_interpolate_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Finally, we'll call the groove method, which will take the interpolated sequence and add groove to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, GrooveLstmDecoder, and hparams:\n",
      "{'max_seq_len': 32, 'z_size': 256, 'free_bits': 48, 'max_beta': 0.2, 'beta_rate': 0.0, 'batch_size': 8, 'grad_clip': 1.0, 'clip_mode': 'global_norm', 'grad_norm_clip_to_zero': 10000, 'learning_rate': 0.001, 'decay_rate': 0.9999, 'min_learning_rate': 1e-05, 'conditional': True, 'dec_rnn_size': [256, 256], 'enc_rnn_size': [512], 'dropout_keep_prob': 0.3, 'sampling_schedule': 'constant', 'sampling_rate': 0.0, 'use_cudnn': False, 'residual_encoder': False, 'residual_decoder': False}\n",
      "INFO:tensorflow:\n",
      "Encoder Cells (bidirectional):\n",
      "  units: [512]\n",
      "\n",
      "INFO:tensorflow:\n",
      "Decoder Cells:\n",
      "  units: [256, 256]\n",
      "\n",
      "INFO:tensorflow:Unbundling checkpoint.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpo9rf_pve/groovae_2bar_humanize/model.ckpt-3061\n",
      "Generated midi file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/groove/groovae_2bar_humanize_00_2019-10-11_201813.mid\n",
      "Generated plot file: /home/alex/Project/hands-on-music-generation-with-magenta/Chapter04/output/groove/groovae_2bar_humanize_00_2019-10-11_201814.html\n"
     ]
    }
   ],
   "source": [
    "# Adds groove to the whole sequence\n",
    "generated_groove_sequence = groove(\"groovae_2bar_humanize\",\n",
    "                                   generated_interpolate_sequence,\n",
    "                                   num_steps_per_sample,\n",
    "                                   num_output,\n",
    "                                   total_bars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a humanize: \n",
    "![MusicVAE humanize 01](docs/img/music_vae_groove_01.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}